# Crawler de Site Web ArchiveChain

Ce dossier contient un exemple avancÃ© de crawler qui peut archiver un site web complet avec toutes ses ressources.

## FonctionnalitÃ©s

- ğŸ•¸ï¸ **Crawling intelligent** avec respect du robots.txt
- ğŸ“„ **Archive complÃ¨te** : HTML, CSS, JS, images, documents
- ğŸ”— **Suivi des liens** avec contrÃ´le de profondeur
- âš¡ **ParallÃ©lisme** configurable pour des performances optimales
- ğŸ›¡ï¸ **Rate limiting** pour respecter les serveurs
- ğŸ“Š **Rapports dÃ©taillÃ©s** avec mÃ©triques et statistiques
- ğŸ¯ **Filtrage avancÃ©** par type de contenu et taille
- ğŸ”„ **Retry automatique** avec backoff exponentiel
- ğŸ’¾ **Sauvegarde progressive** des rÃ©sultats

## Structure

```
website_crawler/
â”œâ”€â”€ README.md              # Ce fichier
â”œâ”€â”€ crawler.py             # Script principal Python
â”œâ”€â”€ crawler.js             # Version JavaScript/Node.js
â”œâ”€â”€ config.yaml           # Configuration exemple
â”œâ”€â”€ requirements.txt       # DÃ©pendances Python
â”œâ”€â”€ package.json          # DÃ©pendances Node.js
â””â”€â”€ examples/             # Exemples de configuration
    â”œâ”€â”€ news_site.yaml
    â”œâ”€â”€ documentation.yaml
    â””â”€â”€ e_commerce.yaml
```

## Installation

### Version Python

```bash
cd examples/advanced/website_crawler
pip install -r requirements.txt
```

### Version Node.js

```bash
cd examples/advanced/website_crawler
npm install
```

## Configuration

CrÃ©ez un fichier `config.yaml` basÃ© sur l'exemple :

```yaml
# Configuration du crawler
crawler:
  max_depth: 3
  max_pages: 1000
  concurrent_downloads: 5
  delay_between_requests: 1.0
  timeout: 30
  user_agent: "ArchiveChain-Crawler/1.0"

# Filtres de contenu
filters:
  allowed_domains: []  # Vide = domaine de dÃ©part uniquement
  allowed_extensions: [".html", ".php", ".pdf", ".doc", ".docx"]
  max_file_size: 50MB
  exclude_patterns:
    - "/admin/"
    - "/private/"
    - "*.zip"
    - "*.exe"

# Options d'archivage
archive:
  include_assets: true
  preserve_javascript: false
  priority: "normal"
  tags: ["crawler", "automated"]

# Configuration ArchiveChain
archivechain:
  api_key: "${ARCHIVECHAIN_API_KEY}"
  api_url: "https://api.archivechain.org/v1"
  network: "mainnet"
```

## Usage

### Python

```bash
# Crawler simple
python crawler.py https://example.com

# Avec configuration personnalisÃ©e
python crawler.py --config config.yaml https://example.com

# Mode dry-run (simulation)
python crawler.py --dry-run --verbose https://example.com

# Avec sauvegarde des rÃ©sultats
python crawler.py --output results.json https://example.com
```

### JavaScript

```bash
# Crawler simple
node crawler.js https://example.com

# Avec options avancÃ©es
node crawler.js --max-depth 5 --concurrent 10 https://example.com

# Mode interactif
node crawler.js --interactive
```

## Exemples de Configuration

### Site d'ActualitÃ©s

```yaml
# examples/news_site.yaml
crawler:
  max_depth: 2
  max_pages: 500
  concurrent_downloads: 3
  delay_between_requests: 2.0

filters:
  allowed_extensions: [".html", ".php"]
  exclude_patterns:
    - "/comments/"
    - "/user/"
    - "?page="
    - "&sort="

archive:
  tags: ["news", "journalism", "current-events"]
  priority: "high"
  metadata:
    category: "News Archive"
    description: "Archive automatique de site d'actualitÃ©s"
```

### Documentation Technique

```yaml
# examples/documentation.yaml
crawler:
  max_depth: 5
  max_pages: 2000
  concurrent_downloads: 8
  follow_external_docs: true

filters:
  allowed_extensions: [".html", ".md", ".pdf", ".txt"]
  allowed_domains: ["docs.example.com", "api.example.com"]

archive:
  include_assets: true
  preserve_javascript: true
  tags: ["documentation", "technical", "reference"]
  metadata:
    category: "Technical Documentation"
    language: "en"
```

### Site E-commerce

```yaml
# examples/e_commerce.yaml
crawler:
  max_depth: 4
  max_pages: 10000
  concurrent_downloads: 5
  respect_robots_txt: true

filters:
  exclude_patterns:
    - "/cart/"
    - "/checkout/"
    - "/account/"
    - "/admin/"
    - "?add-to-cart="
  allowed_extensions: [".html", ".php", ".pdf"]

archive:
  tags: ["e-commerce", "products", "catalog"]
  metadata:
    category: "E-commerce Catalog"
    preserve_prices: true
```

## MÃ©triques et Rapports

Le crawler gÃ©nÃ¨re des rapports dÃ©taillÃ©s incluant :

- ğŸ“Š **Statistiques de crawling** : pages trouvÃ©es, temps total, vitesse
- ğŸ”— **Analyse des liens** : liens internes/externes, liens brisÃ©s
- ğŸ’¾ **Utilisation des ressources** : taille totale, types de fichiers
- âŒ **Erreurs rencontrÃ©es** : 404, timeouts, erreurs de parsing
- ğŸ’° **CoÃ»ts d'archivage** : estimation des coÃ»ts en ARC

### Exemple de Rapport

```json
{
  "crawl_summary": {
    "start_time": "2025-07-24T04:00:00Z",
    "end_time": "2025-07-24T04:15:32Z",
    "duration": "15m32s",
    "pages_crawled": 1247,
    "pages_archived": 1189,
    "errors": 58,
    "success_rate": 95.3
  },
  "content_analysis": {
    "total_size": "2.4 GB",
    "average_page_size": "1.9 MB",
    "file_types": {
      "html": 892,
      "pdf": 156,
      "images": 2341,
      "css": 67,
      "js": 123
    }
  },
  "archive_results": {
    "successful_archives": 1189,
    "failed_archives": 58,
    "total_cost": "1247.50 ARC",
    "average_cost_per_page": "1.05 ARC"
  }
}
```

## Bonnes Pratiques

### Respecter les Serveurs
- Toujours respecter le `robots.txt`
- Utiliser des dÃ©lais appropriÃ©s entre les requÃªtes
- Limiter le nombre de connexions concurrentes
- Utiliser un User-Agent descriptif

### Optimiser les Performances
- Filtrer intelligemment le contenu
- Utiliser la mise en cache pour Ã©viter les doublons
- ParallÃ©liser les tÃ©lÃ©chargements de maniÃ¨re contrÃ´lÃ©e
- Surveiller l'utilisation de la bande passante

### Gestion des Erreurs
- ImplÃ©menter un systÃ¨me de retry avec backoff
- Logger toutes les erreurs pour analyse
- GÃ©rer gracieusement les timeouts
- PrÃ©voir des mÃ©canismes de reprise

## Limitations

- **Rate Limiting** : Respecte les limites des serveurs cibles
- **Contenu Dynamique** : Le JavaScript n'est pas exÃ©cutÃ© par dÃ©faut
- **Authentification** : Ne gÃ¨re pas les sites protÃ©gÃ©s par mot de passe
- **Sites SPA** : Les Single Page Applications nÃ©cessitent une configuration spÃ©ciale

## DÃ©pannage

### Erreurs Communes

**Erreur de connexion SSL**
```bash
# Solution : ignorer les certificats non valides (dÃ©veloppement uniquement)
python crawler.py --ignore-ssl https://example.com
```

**Rate limiting trop agressif**
```yaml
# Augmenter les dÃ©lais dans config.yaml
crawler:
  delay_between_requests: 5.0
  concurrent_downloads: 2
```

**MÃ©moire insuffisante**
```yaml
# RÃ©duire la charge de travail
crawler:
  max_pages: 500
  concurrent_downloads: 2
```

## SÃ©curitÃ©

- Ne jamais exposer vos clÃ©s API dans la configuration
- Utiliser des variables d'environnement pour les secrets
- Respecter les conditions d'utilisation des sites cibles
- Ã‰viter le crawling de sites sans autorisation

## Support

Pour obtenir de l'aide :
- ğŸ“š [Documentation complÃ¨te](https://docs.archivechain.org/crawler)
- ğŸ’¬ [Discord #crawler-support](https://discord.gg/archivechain)
- ğŸ› [Signaler un bug](https://github.com/archivechain/archivechain/issues)